\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Bachelorthesis}
\author{Anders Thuesen}
\date{September 2021}

\begin{document}

\maketitle

\section{Research}
Max Tegmark has written a paper ``Why does cheap and deep learning work so well?`` in which he argues that the reason why neural networks are so good at modelling probabilities is that the real world as we observe it is based around a simple Hamiltonian describing the energy of a system, which is often just a polynomial of relative low degree. It turns out that neural networks are very good at approximating these polynomial functions.

\begin{itemize}
  \item Flattening sometimes makes things more complicated. FFT for instance. Where you could 
\end{itemize}

\section{Introduction}

\[
  \sum_{x=0}^{\infty}{n^2}
\]

\end{document}
