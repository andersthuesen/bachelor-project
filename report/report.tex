\documentclass[a4paper,twoside,11pt]{report} %openright
\usepackage[a4paper, total={6in, 9in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{graphicx}

\title{
  Predictive Coding and Biologically Plausible Neural Networks \\
  \large{Bachelor Project}
}
\author{Anders Bredgaard Thuesen \\ s183926@student.dtu.dk}
\date{January 2022}

\begin{document}
\maketitle

\section*{Abstract}


\newpage

\tableofcontents

\newpage

\section{Introduction}

In the recent years, deep learning has shown impressive results due to the availability of massive parallel compute and huge amounts of data. From the biological inspiration of the neuron to the perceptron where data inputs are weighted, summed together and thresholded, several new modern architectures, like recurrent, residual and transformer neural networks have pushed the limits and achieved state of the art results in speech recognition, computer vision and natural language understanding. Despite of these networks being originally inspired by the brain, the backpropagation (backprop) algorithm for learning the weights and deep learning in general has been criticized for being biologically implausible \cite{BengioLBL15}. This project will primarily be dealing with on of them: The weight transport problem which arises from the way backprop uses the connection weights in both the forward pass (inference) and the backwards parse (calculating the gradients), requiring that both forward and backward connections have symmetric weights and that information is able to flow backwards through the weights. \\
\\
Besides having both philosophical as well scientific interest, studying the computational aspects of how the human brain processes sensory input might lead to great improvements in deep learning and artificial intelligence.
\subsection{Related work}
Write some stuff here about predictive coding, spiking neural networks as such. 

\section{Dataset}
Before we can define our neural network and derive our backpropagation algorithm, we first need to define the dataset on which the network is trained and evaluated. For this we introduce a set of $N$ pairs of datapoints, $\mathbf{x}_i \in \mathbb{R}^k$ and $\mathbf{y}_i \in \mathbb{R}^l$ where $N$ is the size of our dataset, $\mathcal{D} = \left\{(\mathbf{x}_i, \mathbf{y}_i)\right\}$ for $i = 1 \ldots N$. We notice, that both $\mathbf{x}_i$ and $\mathbf{y}_i$ can be vectors of possibly differently dimensions $k$ and $l$ respectively. Here $\mathbf{x}_i$ might represent eg. an image with its horizontal and channel axes flattened into a single one-dimensional vector, as we will encounter later when training on images of handwritten digits in the MNIST dataset \cite{mnist}. In the case of supervised learning, our objective is from $\mathbf{x}_i$ to predict the corresponding label, $\mathbf{y}_i$ which amounts to a single scalar number from 0 to 9 in the MNIST dataset. 

\section{Feed Forward Neural Networks}
% TODO: Write more about feed forward neural networks here.
% - Structure
We define the weight matrices of our feed forward neural network with $L-1$ hidden layers as $\mathbf{W}^{(l)} \in \mathbb{R}^{m \times n}$ for $l = 0 \ldots L$. We initialize the weights using He initialization, where the entries of the matrix $W^{(l)}_{ij}$ is draw from a normal distribution, $\mathcal{N}(0, 2/n)$ \cite{he2015delving}. 

\section{Backpropagation}
The working horse of allmost all modern deep learning models is the back-propagation (aka. backprop) algorithm first popularized for training neural networks by Rumelhart, Hinton \& Williams in 1986 \cite{rumelhart1986learning}. The algorithms solves what is referred to as the \textit{credit-assignment problem}. When learning the parameters of an artificial neural network we would like to know how changing a weight in the network contributes to the total loss, in order to change it in the direction that minimizes the loss. One naive way to do this would be simply to adjust a single random weight slightly, evaluate the new neural network on the dataset and observe the effect on the model loss. If the change leads to a decrease in loss, keep the change, otherwise repeat from the beginning. This would however be very computationally expensive, since the network would have to be evaluated on the entire dataset for each weight in the network. Fortunately, the backprop algorithm achieves this much more efficiently. \\
\\


\begin{equation}
Z = XW^T
\end{equation}
\begin{equation}
A = \sigma(Z)
\end{equation}
\section{Biological constraints}
\section{LIF Neuron model}
\input{sections/LIF.tex}
\section{Predictive Coding}
\section{Variational Inference}

\section{Energy Based Models}

\section{Conclusion}

\newpage
\bibliographystyle{plain}
\bibliography{refs}


\end{document}
