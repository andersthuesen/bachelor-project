\documentclass[a4paper,11pt]{article} %openright
\usepackage[a4paper, total={6in, 9in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}

% Preamble
\DeclareMathOperator{\Pois}{Pois}

\title{
  Predictive Coding and Biologically Plausible Neural Networks \\
  \large{Bachelor Project}
}
\author{Anders Bredgaard Thuesen \\ s183926@student.dtu.dk}
\date{January 2022}

\begin{document}
\maketitle
\thispagestyle{empty}
\section*{Abstract}

\newpage
\tableofcontents
\thispagestyle{empty}

\newpage

\setcounter{page}{1}

\section{Introduction}
\subsection{Motivation}
In the recent years, deep learning has shown impressive results due to the availability of massive parallel compute and huge amounts of data. From the biological inspiration of the neuron to the perceptron where data inputs are weighted, summed together and thresholded, several new modern architectures, like recurrent, residual and transformer neural networks have pushed the limits and achieved state of the art results in speech recognition, computer vision and natural language understanding. Despite of these networks being originally inspired by the brain, the backpropagation (backprop) algorithm for learning the weights and deep learning in general has been criticized for being biologically implausible \cite{BengioLBL15}. This project will primarily be dealing with on of them: The weight transport problem which arises from the way backprop uses the connection weights in both the forward pass (inference) and the backwards parse (calculating the gradients), requiring that both forward and backward connections have symmetric weights and that information is able to flow backwards through the weights. \\
Besides having both philosophical as well scientific interest, studying the computational aspects of how the human brain processes sensory input might lead to great improvements in deep learning and artificial intelligence.

\subsection{Related work}
Several attempts has been made to make modern deep learning more biologically plausible. These can be divided into two types of categories. The first category consists of methods that try to optimize the inference on low-powered neuromorphic hardware such as the Intel Loihi or IBM TrueNorth chips, by converting existing neural network architectures into their spiking counterparts. The other category consists of methods that aim to make the learning phase biologically plausible by only relying on local weight updates in order to optimize for some objective. This is in alignment with the Hebbian learning theory from the neuroscience litterature which states that the synaptic plasticity is only dependent on the pre- and post-synaptic activity, possibly modulated through some global signaling mechasnism (ex. dopamine). One example hereof is the work by Bengio et al. on Continual Equilibrium Propagation \cite{EqProp}. 

\subsection{Research questions}
The project will address the following three research questions:
\begin{itemize}
  \item According to the current literature, what are the biological constraints of biological learning?
  \item To what extent can predictive coding be used to approximate backpropagation under the above mentioned biological constraints?
  \item In what ways can modern deep learning benefit from biological plausible learning algorithms?
\end{itemize}

\section{Classical deep learning}
\subsection{Dataset}
We define our dataset, $\mathcal{D} = \left\{(\mathbf{x}_i, \mathbf{y}_i)\right\}$ for $i = 1 \ldots N$ consisting of $N$ pairs of datapoints, $\mathbf{x}_i \in \mathbb{R}^k$ and $\mathbf{y}_i \in \mathbb{R}^l$ where $N$ is the size of the dataset. We notice, that both $\mathbf{x}_i$ and $\mathbf{y}_i$ can be vectors of possibly differently dimensions $k$ and $l$ respectively. $\mathbf{x}_i$ might represent eg. an image as it is the case with the MNIST dataset used later in this project that consists of 60.000 training examples and 10.000 test examples of 28x28 images depicting handwritten digits and their corresponding labels \cite{mnist}. As the images in the MNIST dataset are two-dimensional, the image has to be flattened into a one-dimensional vector. In the case of supervised learning, the objective is from $\mathbf{x}_i$ to predict the corresponding label, $\mathbf{y}_i$ which would amount to a single scalar number from 0 to 9 in the MNIST dataset. 
\begin{figure}[ht]
  \centering
  \includegraphics[width=280pt]{graphics/mnist.png}  
  \caption{First 10 examples of the MNIST training dataset.}
  \label{fig:MNIST}
\end{figure}



% TODO: Write more about mnist. How many training, test examples? What is the size of each image? Etc. Maybe also a little bit about fashion mnist. 


\subsection{Feedforward neural networks}
Feedforward neural networks (FNN) are considered the simplest kind of neural network where the connections between the nodes does not allow for any cycles or recurrent connections. Feedforward neural networks are divided into several layers, where input data from the first layer is "feed forward" through the so-called hidden layers to the final output layer of the network, considered the prediction of the network. FNNs are typically fully connected networks which entails that every node of the network is connected to every node in the previous layer. One special case of FNNs is that of when there are no hidden layers in the network and the input layer is linearly transformed to the output layer and "activated" through a softmax or sigmoid function, depending on the output of output nodes. In that case, the FNN will correspond to (multinomial) logistic regression. This correspondance incentivises the use of activation functions  after each linear transformation of the layers, as the network would otherwise not be able to describe non-linearities in the data. Historically, the sigmoid activation function $\sigma(z) = (1 + \exp(-z))^{-1}$ has been the go-to activation function, but recently the rectified linear unit, $\textrm{ReLU}(z) = \max(0, z)$, has become the de facto standard. \\
\\*
A feedforward neural network with $L-2$ hidden layers is parametarized by the weight matrices $\mathbf{W}^{(l)} \in \mathbb{R}^{m \times n}$ and biases $\mathbf{b}^{(l)} \in  \mathbb{R}^{m}$ for $l = 2 \ldots L$ where $n$ is the input dimension of the layer and $m$ the output dimension. A feedforward pass from layer $l-1$ to layer $l$ is given by $\mathbf{a}^{(l)} = \sigma(\mathbf{z}^{l})$ where $\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}$ and $\sigma$ is the chosen activation function. By letting the initial activation $\mathbf{a}^{(1)} = \mathbf{x}_i$ one can consider the final activation of the network the prediction of the network $\mathbf{\hat{y}}_i = \mathbf{a}^{(L)}$. \\
\\*
We initialize the weight matrices using Kaiming He initialization, where the entries of the matrix $W^{(l)}_{ij}$ are drawn from a normal distribution with zero mean and $\sqrt{2/n}$ standard deviation as this will help reduce vanishing and exploding gradient problem by keeping the variance in each layer equal when using ReLU activation. \cite{he2015delving}


\subsection{Back-propagation}
\label{sec:backprop}
% TODOS
% - Write more about other alternatives to backpropagation 
% - Write stuff about the hadamard product.
% - Fix transposes and stuffs in equations
The working horse of allmost all modern deep learning models is the back-propagation (aka. backprop) algorithm first popularized for training neural networks by Rumelhart, Hinton \& Williams in 1986 \cite{rumelhart1986learning}. The algorithms solves what is referred to as the \textit{credit-assignment problem}. When learning the parameters of an artificial neural network we would like to know how changing a weight in the network contributes to the total loss, in order to change it in the direction that minimizes the loss. One naive way to do this would be simply to adjust a single random weight slightly, evaluate the new neural network on the dataset and observe the effect on the model loss. If the change leads to a decrease in loss, keep the change, otherwise repeat from the beginning. This would however be very computationally expensive, since the network would have to be evaluated on the entire dataset for each weight in the network. Fortunately, the backprop algorithm achieves this much more efficiently as we will see in the following section. \\
\\*
Back-propagation is an efficient method for calculating the weight updates that minimizes some loss function, $\mathcal{L}(\hat{\mathbf{y}}_i, \mathbf{y}_i)$, which measures the difference between the predicted output of the network, $\hat{\mathbf{y}}_i$, and the true output, $\mathbf{y}_i$. Examples of loss functions are squared error $\sum{\frac{1}{2}(\hat{\mathbf{y}}_i - \mathbf{y}_i)^2}$, typically used for regression and categorical cross entropy $-\sum{\mathbf{y}_i \cdot \log(\hat{\mathbf{y}}_i)}$ for classification. At its core, the back-propagation algorithm is simply applying the chain rule on the partial derivate of the loss function with respect to the parameters and realizing that a lot of computation can be reused or "back propagated" in order to calculate the weight and bias updates for earlier layers. It is therefore necessary that the loss function is differentiable with respect to the prediction variable. Some alternatives to back-propagation exist such as Direct Feedback Alignment \cite{noekland2016direct}, but is not commonly used in practice.  

To demonstrate the efficiency of the back-propagation algorithm, one can consider the last and second to last layers of the network. For now, the demonstration will only consider the weights as the bias terms can integrated into the weight matrices by extending the output dimension $n$ by 1 and appending a 1 to the $\mathrm{a}^{(l)}$ vectors resulting which will give an equivalent result. Applying the chain rule on the partial derivative of the loss function wrt. $\mathbf{W}^{(L)}$ yields
\begin{equation} \label{eq:dLdW1}
  \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(L)}}  
= \underbrace{
    \frac{\partial \mathcal{L}}{\partial \mathbf{a}^{(L)}}
    \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}}
  }_ {\delta^{(L)}}
  \frac{\partial \mathbf{z}^{(L)}}{\partial \mathbf{W}^{(L)}}
= \underbrace{
    \mathcal{L}^\prime (\hat{\mathbf{y}}_i) \sigma^\prime(\mathbf{z}^{(L)})
  }_{\delta^{(L)}}
 \mathbf{a}^{(L-1)}
\end{equation}
whose factors can be divided into the error term, $\delta^{(L)}$, and activation in the previous layer, $\mathbf{a}^{(L-1)}$. Yet again, applying the chain rule on the partial derivative of the loss function, but this time wrt. $\mathbf{W}^{(L-1)}$ hints at the source of its efficiency:
\begin{equation}
\begin{split}
    \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(L-1)}}
  % &= \frac{\partial \mathcal{L}}{\partial \mathbf{a}^{(L)}}
  %   \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{a}^{(L-1)}}
  %   \frac{\partial \mathbf{a}^{(L-1)}}{\partial \mathbf{z}^{(L-1)}}
  %   \frac{\partial \mathbf{z}^{(L-1)}}{\partial \mathbf{W}^{(L-1)}}
 &= \underbrace{
   \frac{\partial \mathcal{L}}{\partial \mathbf{a}^{(L)}}
   \frac{\partial \mathbf{a}^{(L)}}{\partial \mathbf{z}^{(L)}}
 }_{\delta^{(L)}}
   \frac{\partial \mathbf{z}^{(L)}}{\partial \mathbf{a}^{(L-1)}}
   \frac{\partial \mathbf{a}^{(L-1)}}{\partial \mathbf{z}^{(L-1)}}
   \frac{\partial \mathbf{z}^{(L-1)}}{\partial \mathbf{W}^{(L-1)}}
  = \underbrace{
    \delta^{(L)}
    \mathbf{W}^{(L)}
    \sigma^\prime(\mathbf{z}^{(L-1)})
  }_{\delta^{(L-1)}}
  \mathbf{a}^{(L-2)}
\end{split}
\end{equation}
Though the derivation above only considers $\mathbf{W}^{(L)}$ and $\mathbf{W}^{(L-1)}$ it is the general case that
\begin{equation} \label{eq:delta-terms}
  \delta^{(L)} = \mathcal{L}^\prime (\hat{\mathbf{y}}_i) \sigma^\prime(\mathbf{z}^{(L)}), 
  \hspace{10pt}
  \delta^{(l)} =  \delta^{(l+1)}\mathbf{W}^{(l+1)}\sigma^\prime(\mathbf{z}^{(l)})
\end{equation}
which can be computed using a dynamic programming approach to update the weight parameters using gradient descent with learning rate $\alpha$:
\begin{equation} \label{eq:weight-update}
  \mathbf{W}^{(l)} = \mathbf{W}^{(l)} -\alpha \delta^{(l)} \mathbf{a}^{(l-1)}.
\end{equation}
The weight is updated in the opposite direction (hence the negation) of the gradient as the aim is to minimize the loss function. By reusing the computation of the error terms, $\delta^{(l)}$, backprop is able to very efficiently compute the weight updates to minimize the global loss function. In theory the weights should be updated according to all datapoints in the dataset to maximize the likelihood, however in practice stochastic gradient descent is used with mini-batches of datapoints that stochastically resembles the overall distribution of the dataset. 

\subsection{Batch normalization}
When training deep neural networks, it is often necessary to normalize the inputs to the network in obtain good results. By normalizing the input variables to the network the gradient of the loss function is more likely to point in the direction of the local minima resulting in more efficient and stable training. According to the authors \cite{ioffe2015batch}, batch normalization has the effect of reducing ``internal covariate shift'' of the networks intermediate representations. As the weights and biases of each layer in the network is updated, the distribution of the internal representations tends to change aswell. This has the effect of slowing down learning and decrease the chance of converging to a good local minima. By normalizing each mini-batch, batch normalization is able to accelerate deep learning by enabling higher learning rates and fewer training steps. Batch normalization works by using a combination of the mean and variance of the current mini-batch and a running mean and variance. At training time, batch normalization standardizes the mini-batch according to
\begin{equation}
  \hat{\mathbf{x}}_{i}^{(l)} = \frac{\mathbf{x}^{(l)}_{i}-\mu^{(l)}_{B}}{\sqrt{{\sigma^{2}}^{(l)}_{B}+\epsilon}}
\end{equation}
where $\mu_{B}$ is the mean of the batch, $\sigma_{B}^{2}$ is the variance of the batch and $\epsilon$ is some small value for numerical stability (typically $10^{-5}$). Batch norm then outputs an affine transformation using learned parameters $\gamma$ and $\beta$
\begin{equation}
  \mathbf{y}_{i} = \gamma \hat{\mathbf{x}}_{i}+\beta \equiv \mathrm{BN}_{\gamma, \beta}\left(\mathbf{x}_{i}\right).
\end{equation}
During the training fase, the mean and variance of the dataset is estimated by the means of an exponential moving average
\begin{equation}
  \begin{split}
    \mu_\text{mov} &\leftarrow \alpha \mu_\text{mov} + (1 - \alpha)\mu_{B} \\
    \sigma^2_\text{mov} &\leftarrow \alpha \sigma^2_\text{mov} + (1 - \alpha)\sigma^2_{B} \\
  \end{split}
\end{equation}
where $\alpha=0.1$ is the default value. At inference time, batch normalization uses the dataset mean and variance estimations and affine scaling and translation parameters to normalize the intermediate representations
\begin{equation} \label{eq:batch-norm}
  \mathbf{y}_i = \frac{\gamma}{\sqrt{\sigma^{2}_\text{mov} +\epsilon}} \cdot \mathbf{x}_i +\left(\beta - \frac{\gamma \mu_\text{mov}}{\sqrt{\sigma^{2}_\text{mov}+\epsilon}}\right).
\end{equation}

\section{Biologically plausible deep learning}
% TODO
% - Maybe write a little bit about what this section is about?
% - Maybe something about alternatives to predictive coding that are also biologically plausible?

The following section will focus on biologically plausible deep learning and what that entails. There are many aspects of being biological plausible, however this project will only be considering those that are computationally relevant for deep learning. Initially to gain an understanding of biology a brief summary of the neuron is presented. From this biological constraints are defined and compared with current deep learning approaches. Subsequent sections will describe alternative, more biologically plausible methods for training and evaluating artificial neural networks. This project will primarily focus on methods presented in the two papers \textit{Spiking Deep Networks with LIF Neurons} by Eric Hunsberger et al. \cite{hunsberger2015spiking} and \textit{Can the Brain Do Backpropagation? - Exact Implementation of Backpropagation in Predictive Coding Networks} by Yuhang Song et al. \cite{PredictiveCodingNetworks}. The endgoal of this project is to compare these methods with a unification of the two on the supervised task of predicting the label of MNIST digits. Hopefully, the combination of these methods will shine some light on the intersection between artificial and biological neural networks and provide a foundation for further research.

\subsection{Biological neurons} \label{sec:neurons}
This section aims to briefly describe the biological neuron from a highlevel perspective to give an intuition of how processing is happening in the human brain. Hopefully this will lead to a better understanding of what biologically plausibility encapsulates. It is by no means fully comprehensive nor capturing all the nuanced details of the biological processes. \\
\\*
A neuron is an electrically excitable cell capable of communicating with other neurons by sending electrical signals called action potentials or spikes. Most often, multiple spikes are generated in sequence, called a spike train. When spikes are generated the neuron is said to be \textit{firing}. The neuron has a cell body called the soma which is the central element in processing. From the soma root like structures called dendrites extrude, which capture signals from other nearby neurons. Receiving spikes from other presynaptic neurons will lead to polerization of the cell and trigger the neuron to spike itself if the input spikes exceed a certain threshold. As the neuron spikes an action potential will travel from the soma, through the axon and to the synaptic terminal where it will cause the release of neurotransmitter molecules to the postsynaptic neurons dendrites. Neurons are usually in computational models described by their membrane potential or voltage which arises from the fact that neurons maintain a voltage across its membrane. It achieves this by actively regulating the intracellular concentration of charged potassium, sodium and calcium ions through ion pumps until a resting potential is reached (around -70mV). When neurotransmitter molecules bind to receptors on the dendrites, it will cause various types of ion channels to open. This will result in a change in the ionic concentration and thereby membrane potential of the neuron. Once the membrane voltage reaches a certain level called the threshold voltage (around -55mV) voltage-gated ion channels open and ions flood in to the cell, further increasing the membrane voltage to form a chain reaction. This reaction is what causes neurotransmitter to be released at the synaptic terminal. In other words, the neuron will spike. After spiking a phase of hyperpolerization called the refractory period might occur. In this phase, ion pumps decrease the membrane potential beyond the resting potential and prevents the neurons from spiking. \\
\\*
It is believed that the brain learns both by forming new connections between neurons and by varying the amount of neurotransmitter receptors resulting in strengthening or weakening of existing connections. In the book, The \textit{Organization of Behaviour} by Donald Hebb \cite{hebb1949}, he postulates that the pre- and postsynaptic activity drive this change, known as Hebb's postulate or Hebbian learning: 
\begin{quotation}
"When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased."
\end{quotation}
This postulate is congruent with the learning mechanism of spike-timing dependent plasticity (STDP) where changes in synaptic strength depend on the relative timings of the pre- and postsynaptic spikes, which has been observed to happen in the brain\cite{Bi10464}.
% Inhibitory

% Note on the level of abstraction.

\begin{figure}
  \centering
  \includegraphics[width=220pt]{graphics/bio-neuron.png}
  \caption{Illustration of biological neuron by BruceBlaus - Own work, CC BY 3.0 \protect\footnotemark}
  \label{fig:bio-neuron}
\end{figure}
\footnotetext{\href{https://commons.wikimedia.org/w/index.php?curid=28761830}{https://commons.wikimedia.org/w/index.php?curid=28761830}}


\subsection{Biological constraints} \label{sec:biological-constraints}
To understand how the biological neurons process information and give rise to the emergent phenomenon of intelligence it is of interest to study the natural biological and physical limitations that they exhibit. One interesting question is to whether the perticular biological implementation of the neural networks in the brain are fundamental for the existense of intelligence or if it is rather a single instance in the set of possible configurations. For now, the focus will be on defining the constraints of the only tried-and-testet implementation we know of; the biological neuron. Neurons communicate using spikes of roughly the same amplitude, not known to encode any information, but is able to dynamically modulate the rate and timings of spikes depending on the presynaptic stimulation. This leads to our first \textbf{constraint of spikes} for communication. Due to the anatomical structure of neurons communication is unidirectional in the direction from the cell body, along the axon and to the synaptic terminal. No spiking signal is known to travel in the reverse direction from post-synaptic dendrites to pre-synaptic terminal. Thus we introduce a \textbf{constraint of unidirectionality}. Neurons primarily communicate with nearby neurons. Efficiency wise, it makes sense for the brain to place functionally dependent regions nearby to save energy in signal transmission. This principle of locality is also found in Hebb's postulate and STDP. We therefore say that biological learning is \textbf{constrained by locality}.

\subsubsection{Biological violations of backprop and deep learning}
In their 2016 paper, Bengio et al \cite{BengioLBL15}. raises 6 problems regarding the biologically plausibility of back-propagation and artificial neural networks which are nicely summarized and expanded upon in the PhD thesis by Hunsberger \cite{Eric2018} from which this project will use the same naming of the problems. To illustrate the problems more clearly in terms of dicussions in previous sections (\ref{sec:backprop}, \ref{sec:neurons}) the problems have been reformulated as follows.

\begin{enumerate}
  \item \textbf{Weight transport problem}: Back-propagation uses the same connection weights in both the forward pass for prediction and in the backwards pass when calculating the gradients as seen in equation \ref{eq:delta-terms}. A well established fact of biological neurons is that of spikes travilling unidirectionally from the soma, through the axons and across the synapses as neurotransmitter chemicals. Backprop does not seem to comply with this constraint. 
  \item \textbf{The derivative problem}: The derivative problem problematises the need for the derivative of the activation function (ex. sigmoid or ReLU used in the forward pass) in order to back-propagate the error signal as it is also the case in equation \ref{eq:delta-terms}. It is not known that biological neurons can do this.
  \item \textbf{The linear feedback problem}: The linear feedback problem is due to the fact that neurons communicate in a non-linear fashion making the linear dependence on the feedback error term, $\delta^{(l)}$, in equation \ref{eq:weight-update} difficult to implement for biological neurons. 
  \item \textbf{The spiking problem}: Biological neurons communicate with spikes whereas artificial neural networks use real values. It might be possible in a sense to communicate both positive and negative values stochastically by encoding the value in the spike rate with a combination of excitory and inhibitory neurons, however back-propagation depends on differentiable activation functions to work.
  \item \textbf{The timing problem}: In the back-propagation algorithm, forwards and backwards passes are run alternatingly and sequantially, which is not known to be happening in the human brain. In constrast, neurons function asynchrously when activated by their inputs and spikes travel between neurons with some propagation delay, making it difficult for biological neurons to implement backprop as activations and their derivates should stay constant when performing weight updates. 
  \item \textbf{The target problem}: It is unclear how labels such as those in the MNIST dataset could be present in the brain. The brain seems to make sense of the world by itself without needing constant supervision. Humans generally only need a few examples in order to identify different objects. Though supervised learning has shown the most impressive results thus far, recent focus on unsupervised or self-supervised learning has startet to show its promise, which might better explain how the brain learns. 
\end{enumerate}

\newpage


% From a highlevel perspective 

% These methods are comprised of spiking neural networks for making the communication between neurons 

% Discuss how these methods aim to overcome the current biological constraints defined in section 3.2

\subsection{Spiking neural networks}
Spiking neural networks (SNNs) are a perticular type of network that uses spikes for inter-neuron communication. In that way they more closeley mimic biological neural networks, but share their structure and dense connectivity with feedforward neural networks. Spiking neurons are temporal in their dynamics in the sense that they are integrating input spikes over time. When the threshold membrane potential is exceeded a spike is sent to the connected neurons in the next layer. The implementation of SNNs requires a perticular neuron model of the action potential often modelled as a first order differential equation describing the exact dynamics. Several different neuron models exists ranging from simple and compute efficient models like the Integrate-and-Fire (IF) model that only captures the coarse dynamics of biological neurons to more nuanced models like the Hodgkin-Huxley model. We will be using a variation of the IF neuron model which leaks membrane potential over time, called the Leaky-Integrate-and-Fire model. This particular model has the advantage of being relatively biologically plausible yet still being very efficient to compute. 

\subsubsection{Training spiking neural networks}
There exist several methods for training SNNs such SpikeProp \cite{Boht2000SpikePropBF} which uses an algorithm akin to backprop directly on the generated spikes using surrogate gradient functions or modelling approaches where the spiking dynamics of neurons is defined using recurrent networks and trained using backprop through time, like snnTorch \cite{eshraghian2021training}. This project will use a method of shadow training where a ``shadow'' ANN will be trained using spike-rate coding and later run as spiking neural network at inference time using the learned weights from the ANN. This has the advantage of leveraging existing deep learning infrastructure and optimization techniques while being deployable on low-powered or analog hardware. 

\subsubsection{Leaky-Integrate-and-Fire neurons}
% Maybe a note about units?
Leaky-Integrate-and-Fire neurons were inspired by the fact that biological neurons seem to lose voltage over time as ions leak through the membrane. Computationally this might be understood as a temporal filter that decreases the effect on the action potential of spikes happened longer time back in the past. The Leaky-Integrate-and-Fire (LIF) neuron model can be described by the following differential equation:
\begin{equation}
  C \frac{d V}{dt}=J(t)-\frac{V}{R}
\end{equation}
where $V$ is the voltage across the membrane, $R$ the membrane resistance and $J(t)$ the input current to the neuron at time $t$. The neuron will fire when the membrane voltage reaches a threshold of $V_\text{th}$ and reset to $V_\text{rest}$ in a refractory period of $t_\text{ref} = 0.004$. If the applied input current is not large enough to make the neuron spike, the membrane voltage will naturally decay to $V_\text{rest}$. Hunsberger \cite{Eric2018} shows how the above can be normalized and rewritten in simpler terms as:
\begin{equation}
  \tau_{RC} \frac{dv(t)}{dt} = j(t) - v(t) 
\end{equation}
where $\tau_{RC}$ is the membrane time constant and $v(t)$ and $j(t)$, resembling the membrane voltage and input current, are now unitless. Generally $\tau_{RC}$ is in the range in the tens of miliseconds for biological neurons. We use $\tau_{RC}=0.02$. The implication of this rewrite is that the neuron now spikes at $v(t) = v_\text{th} = 1$ before resetting to $v_\text{rest} = 0$ and entering the refractory period. Forcing the input current to be static by letting $j(t) = j$ and solving the differential equation allows us to describe the membrane voltage over time with constant input current:
\begin{equation}
  v(t) = (v_0 - j) e^{-t/\tau_{RC}} + j
\end{equation}
where $v_0$ is the voltage at $t=0$ which most often is set $v_0 = V_\text{rest}$. % Plotting $v(t)$ with different values of $j$ and $v_0$ can be seen in figure \ref{fig:lif-voltage}.
% \begin{figure} 
%   \centering
%   \includegraphics[width=350pt]{graphics/lif-voltage.png}
%   \caption{Voltage of LIF neuron over time for different values of $j$ and $v_0$.}
%   \label{fig:lif-voltage}
% \end{figure}
It is now possible to derive the spike rate which for input current 
when the input current exceed the threshold $j>V_\text{th}$ by setting $v(t) = V_\text{th}$ and solving for $r = t^{-1}$ to get.
\begin{equation}
  r(j) = \begin{cases} 
      \left[t_\text{ref} - \tau_{RC} \log \left(1-\frac{V_{\text {th }}}{j}\right) \right]^{-1} & \text{if } j > V_\text{th} \\
      0 & \text{otherwise}
  \end{cases}
\end{equation}
Notice that $t_\text{ref}$ is added to the time-to-spike, to incorporate the effect of the refractory period. Using the rate function directly as activation function would lead to problems as its derivative goes to infinity when $j \to 0_{+}$. Hunsberger et al. therefore introduces a smoothed rate function using the softplus function $\rho(x) = \gamma \log(1 + \exp(x / \gamma))$ that works as a smooth $\max$ function and does in fact becomes equivilant when $\gamma \to 0$.

\begin{equation}
  r_\rho(j) = \left[t_\text{ref} - \tau_{RC} \log \left(1 + \frac{ V_{\text{th}} }{ \rho(j - V_\text{th})}\right) \right]^{-1}
\end{equation}
A plot of the original and smoothed rate functions can be seen in figure \ref{fig:rate-function}.

\begin{figure}
  \hfill
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphics/smooth-rate.png}
    \caption{Interleaved plot of original rate function, $r(j)$ (orange), and smoothed version, $r_\rho(j)$ (blue).} 
    \label{fig:rate-function}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphics/poisson.png}
    \caption{Theoretical (orange) and simulated spike rates (blue) as functions of input current.}
    \label{fig:poisson-rate}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphics/rate-scaling.png}
    \caption{Scaling of the rate function in the interval $[0; 10]$ to mimic ReLU.}
    \label{fig:rate-scaling}
  \end{subfigure}
  \caption{Plots of smoothed, poisson coded and ReLU-like rate functions, respectively.}
\end{figure}



\subsubsection{Poisson rate coding}
As SNNs operate on spiking inputs, continuous input variables have to be converted to stochastic spiking variables. Such can be accomplished with Possion rate coding where the value of the continuous variable is interpreted as the mean spike rate of the Poission distribution with the following parameters:
\begin{equation}
  s_i^{(t)} \sim \Pois \left( \gamma = \frac{x_i}{t_\text{ref}} dt \right)
\end{equation}
where $dt$ is the discrete timestep used for simulation. This formulation requires that input variables $x_i$ are normalized to values between 0 and 1 as division with $t_\text{ref}$ maps $1$'s to the maximum spike rate $r_\text{max} = 1 / t_\text{ref}$. Disadvantageously, poission rate coding causes the stochastic signal to contain a relativ large amount of noise in the signal as seen in figure \ref{fig:poisson-rate}.


\subsubsection{Low-pass alpha-filter}
When modelling biologically realistic neural networks it is equally important to model the synapses as the neural dynamics. Synapses are known to act as low-pass filters on the transmitted spikes. Hunsberger and Eliasmith \cite{hunsberger2015spiking} proposes a simple exponential low-pass filter
\begin{equation} \label{eq:alpha-filter}
  \alpha(t) = \frac{t}{\tau_{s}} e^{-t / \tau_{a}}
\end{equation}
where $\tau_s$ is some time-constant. 



\subsection{Shadow training spiking neural networks}
The method of shadow training spiking neural networks amounts to the task of training a standard neural network using back-propagation, incorporating the rate-function of the chosen dynamical neuron model as activation function, whereby a spiking neural network can be constructed with the learned weight and bias parameters. However, doing so is in some cases not always quite as trivial. This section will go into the details of shadow training feedforward neural networks and the tricks for successfully doing so. 

\subsubsection{Activation scaling}
When training neural networks using the analytical spiking rate as activation function, we found that down-scaling the activation function drastically improved the converge of the network. The reason for this result can be explained by the combination of the Kaiming He initialization of the weights which keeps the variance of the intermediate representations in the network close to $1$ when used in conjunction with ReLU activation. By scaling the rate function with a factor of $0.05$ the rate function and its derivative will more closely mimic that of the ReLU in the interval $[0; 10]$ as seen in figure \ref{fig:rate-scaling}. 

\subsubsection{Rate estimation using alpha-filter}
By using a low-pass alpha-filter as defined in equation \ref{eq:alpha-filter} it is possible to adaptively estimate the underlaying firing rate of a sequence of spikes. This is useful as our rate function expects a current as input which is correlated with the instantatious pre-synaptic firing rate. We found that using an alpha filter with time-constant $\tau_s = 1.18 \cdot 10^{-3}$ most precisely determined the underlaying firing rate. 

\subsubsection{Fusing batch normalization}
In the process of converting shadow trained ANNs to the SNNs, custom modules such as batch normalization layers has to be converted to their spiking counterparts. Fortunately, if used in between the linear layer and activation, it is possible to ``fuse'' batch normalization into the weight and bias paramters of the predecessing linear layer. By replacing $\mathbf{x}_i$ in equation \ref{eq:batch-norm} with the transformation from our linear layer $\mathbf{z}_i^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)}_i  + \mathbf{b}^{(l)}$ 
\begin{equation}
    \mathbf{y}_i = \frac{\gamma}{\sqrt{\sigma^{2}_\text{mov} +\epsilon}} \cdot \left( 
      \mathbf{W}^{(l)} \mathbf{a}^{(l-1)}_i  + \mathbf{b}^{(l)}
    \right) + \left(\beta - \frac{\gamma \mu_\text{mov}}{\sqrt{\sigma^{2}_\text{mov}+\epsilon}}\right) \\
\end{equation}
it is possible to derive the ``fused'' weight and bias, parameters as follows:
\begin{equation}
    \overline{\mathbf{W}}^{(l)} = \operatorname{diag} \left( \frac{\gamma}{\sqrt{\sigma^{2}_\text{mov} +\epsilon}} \right) \mathbf{W}^{(l)}, \hspace{10pt} \overline{\mathbf{b}}^{(l)} =  \operatorname{diag} \left( \frac{\gamma}{\sqrt{\sigma^{2}_\text{mov} +\epsilon}} \right) \mathbf{b}^{(l)} + \left(\beta - \frac{\gamma \mu_\text{mov}}{\sqrt{\sigma^{2}_\text{mov}+\epsilon}}\right).
\end{equation}

\subsubsection{Testing spiking neural networks}
It is crucial and foundational to machine learning when constructing models to test how well they generalize on new data that the model has not been exposed to before. When testing on a well-balanced catagorical dataset, one might simply want to measure the accuracy of the model. For neural networks trained with softmax activation in the last layer, the output of the model can be seen as probability distribution of the input belonging to a certain class. Although this interpretation is useful, it is enough to take the argmax of the logits (inputs to the softmax) when trying to predict the most probable class. For spiking neural networks there are different options for determining the prediction of the network. One might predict the class of the first neuron that spikes if the time to prediction is critical. However, this method might not yield the best result as noise in the input signal might lead a neuron, not representative of the true class, to spike first. To evaluate our spiking network, we predict the class of the neuron that had the most spikes in a period of one second.

\subsection{Predictive Coding}
Predictive coding is hierarchial theory of perception in the brain where higher level cortical areas ``explain away'' lower-level visual input and only discrepancies hereof are propagated up for further processing. The theory sparked great interest in the neuroscientific community when Rao and Ballard published their seminal 1999 paper \cite{raoballard1999} showing how hierarchial predictive coding models were able explain neurological phenomenon like endstopping and other extra-classical receptive field effects where the surroundings modulate the response of a receptive field. In the predictive coding theory, the brain is seen as a generative model, able to restore the visual input of the retina by modelling the three dimensional structure of the world. This could also be formulated as inferring the underlaying hidden state of the world, $s_i$, causing the sensations, $o_i$, by modelling $p(o_i | s_i)$. Hence, predictive coding share core ideas with the bayesian brain theory. Though predictive coding was originally only ment to explain the perception, it has been suggested as unifying theory of the brain \cite{millidge2021predictive}. This section will focus on the correspondance between a special instance of predictive coding dubbed zero inference-learning (Z-IL) where the minimization of error units is equivalent to back-propagation as shown by Song et al \cite{PredictiveCodingNetworks}.

\subsubsection{Predictive coding networks and inference learning}
\begin{figure}
  \centering
  \includegraphics[width=300pt]{graphics/predictive-coding-network.pdf}
  \caption{Predictive coding network with 2 input nodes, 3 hidden nodes and 2 output nodes.}
  \label{fig:predictive-coding-network}
\end{figure}
Predictive coding networks are very similar to feedforward neural networks but include error nodes, $\epsilon_{i,t}^{(l)}$, between value nodes (as illustrated in figure \ref{fig:predictive-coding-network}) that encode the difference between a value node, $x_{i,t}^{(l)}$, and its prediction, $u_{i,t}^{(l)}$:
\begin{equation}
  \mathbf{\epsilon}_t^{(l)} = \mathbf{x}_t^{(l)} - \mathbf{u}_t^{(l)},\hspace{10pt}  \mathbf{u}_t^{(l)} = \mathbf{W}^{(l)} \sigma(\mathbf{x}_t^{(l-1)}) + \mathbf{b}^{(l)}
\end{equation}
 Inference learning (IL) is a particular algorithm introduced by Song et al. \cite{PredictiveCodingNetworks} to learn the parameters of predictive coding networks for both supervised and un-supervised learning tasks. The algorithm runs in two phases; an inference phase and a learning phase. To train the network supervised, the input and target nodes are clamped to the input and target variables that the network should learn a mapping between. Then in the inference phase, value nodes are inferred such that they minimize the total free energy of the network:
\begin{equation}
  F = \sum_{l=2}^{L}{
    {\mathbf{\epsilon}^{(l)}}^T\mathbf{\epsilon}^{(l)}.
  }
\end{equation}
To minimize the error nodes, the value nodes are updating according to the following rules;
\begin{equation} \label{eq:update-value-nodes}
  \dot{\mathbf{x}}_t^{(l)} = \begin{cases}
    0 & \text{if } l=1 \\
     -\mathbf{\epsilon}_t^{(l)} + \sigma^\prime(\mathbf{x}_t^{(l)}) ( \mathbf{\epsilon}_t^{(l+1)} \mathbf{W}^{(l+1)} )  & \text{if } l = 2 \ldots L-1 \\
    - \mathbf{\epsilon}_t^{(l)} & \text {if } l = L \text{ during prediction} \\
    0 & \text {if } l = L \text{ during learning}
  \end{cases}
\end{equation}
using Eulers method such that $\mathbf{x}_{t+1}^{(l)} = \mathbf{x}_{t}^{(l)} + \gamma \dot{\mathbf{x}}_t^{(l)}$ where $\gamma$ is the integration step size. Once value nodes have converged to an equilibrium at time $t=T$, the parameters are updated to yet again minimize the free energy:
\begin{equation} \label{eq:il-weight-update}
  \Delta \mathbf{W}^{(l)} = -\alpha \epsilon_T^{(l+1)} \sigma(\mathbf{x}_T^{(l)})
\end{equation}
where $\alpha$ is the learning rate. A possible source of confusion when coming from feedforward neural networks is that predictive coding networks do not propagate activity directly from value node to value node. Instead, value nodes should be considered free variables that are updated according to equation \ref{eq:update-value-nodes}. Interestingly, after the inference phase comes to an equilibrium where $\dot{\mathbf{x}}_T^{(l)} = 0$ and all error nodes become zero, the value nodes will correspond to the activity of the neurons in forward prediction of an classic ANN; $0 = \epsilon_{T}^{(l)}=\mathbf{x}_{T}^{(l)}-\mathbf{u}_{T}^{(l)}  \Rightarrow \mathbf{x}_{T}^{(l)} = \mathbf{u}_{T}^{(l)}$, if only the input node is clamped. Even more intriguing is the fact that the error nodes will become approximately equal to the error terms in the back-propagation algorithm (equation \ref{eq:delta-terms}) when both input and output nodes are clamped 
\begin{equation}
    \mathbf{\epsilon}_T^{(L-1)} = \sigma^{\prime}(\mathbf{x}_{T}^{(L-1)})\left(\mathbf{x}_{T}^{(L)} - \mathbf{u}_T^{(L)} \right), \hspace{10pt} \mathbf{\epsilon}_T^{(l)} = \sigma^{\prime}(\mathbf{x}_{T}^{(l)})\left(\epsilon_{T}^{(l+1)} \mathbf{W}^{(l+1)}\right).
\end{equation}
Here, $\mathbf{x}_{T}^{(L)} - \mathbf{u}_T^{(L)}$ is the derivative of the squared loss $\mathcal{L}(y, \hat{y}) = \frac{1}{2}\left( y - \hat{y}  \right)^2$. 

\subsubsection{Z-IL and equivalence to back-propagation}
Song et al. takes this a step further by showing that a special variant of inference learning, zero-divergence inference learning (Z-IL) gives rise to the exact same weight updates as the back-propagation algorithm \cite{PredictiveCodingNetworks}. They show this by first assuming that every value node, $x_{i,t}^{(l)}$ and prediction $u_{i,t}^{(l)}$ is equal to the corresponding forward activity of the equivalent ANN, after which the integration step size $\gamma$ is set to 1 and weights are scheduled to be updated according to equation \ref{eq:il-weight-update} at time $t=l$. 

\begin{itemize}
  \item Talk about how predictive coding takes the total network error into account when updating the weights. 
  \item Backprop does not but is probably more efficient to compute. Is there any examples of weights that can be learned by predictive coding, but not backprop? Maybe backprop will be able to learn those, but with a much smaller learning rate since predictive coding takes this into account in the inference phase. 
\end{itemize}

\subsubsection{Biological plausability}
\begin{itemize}
  \item Write about the biological plausability.
  \item Update rules are now local, but still require symmetric weights? \begin{itemize}
    \item Maybe this could be solved with local random feedback alignment?
  \end{itemize}
\end{itemize}

\subsubsection{Variational free energy}
Why is this relevant?

\subsection{Variational Inference}
Kriston paper about 

\subsection{Energy Based Models}
\begin{itemize}
  \item What can they be used for in general?
  \item Is predictive coding an instance of this?
  \item How do they handle adverserial attacks?
\end{itemize}

\section{Results}

\section{Discussion}

\section{Conclusion}
\newpage
\bibliographystyle{plain}
\bibliography{refs}


\end{document}
