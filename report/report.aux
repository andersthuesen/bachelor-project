\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{lstm}
\citation{resnet}
\citation{transformers}
\citation{hebb1949}
\citation{noekland2016direct}
\citation{lillicrap2014random}
\citation{BengioLBL15}
\citation{EqProp}
\citation{ContinualEqProp}
\citation{predcoding}
\citation{raoballard1999}
\citation{PredictiveCodingNetworks}
\citation{salvatori2021reverse}
\citation{milladge2020predictive}
\citation{lotter-prednet}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Related Work}{1}{subsection.1.1}\protected@file@percent }
\citation{Boht2000SpikePropBF}
\citation{eshraghian2021training}
\citation{hunsberger2015spiking}
\citation{PredictiveCodingNetworks}
\citation{hunsberger2015spiking}
\citation{BengioLBL15}
\citation{Eric2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Research Questions}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Project Outline}{3}{subsection.1.3}\protected@file@percent }
\citation{mnist}
\@writefile{toc}{\contentsline {section}{\numberline {2}Deep Learning}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Mathematical Notation}{4}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Dataset}{4}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Maximum Likelihood Estimation}{4}{subsection.2.3}\protected@file@percent }
\newlabel{sec:MLE}{{2.3}{4}{Maximum Likelihood Estimation}{subsection.2.3}{}}
\citation{Goodfellow-et-al-2016}
\newlabel{eq:negative-log-likelihood}{{4}{5}{Maximum Likelihood Estimation}{equation.2.4}{}}
\newlabel{eq:regression-loss-deriv}{{6}{5}{Maximum Likelihood Estimation}{equation.2.6}{}}
\newlabel{eq:softmax}{{7}{6}{Maximum Likelihood Estimation}{equation.2.7}{}}
\newlabel{eq:softmax-derivative}{{12}{6}{Maximum Likelihood Estimation}{equation.2.12}{}}
\newlabel{eq:partial-log-softmax}{{15}{7}{Maximum Likelihood Estimation}{equation.2.15}{}}
\newlabel{eq:cross-entropy-loss-deriv}{{22}{7}{Maximum Likelihood Estimation}{equation.2.22}{}}
\citation{he2015delving}
\citation{rumelhart1986learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Feed-forward Neural Networks}{8}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Weight initialization}{8}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Back-propagation}{8}{subsection.2.6}\protected@file@percent }
\newlabel{sec:backprop}{{2.6}{8}{Back-propagation}{subsection.2.6}{}}
\citation{noekland2016direct}
\newlabel{eq:dLdW1}{{23}{9}{Back-propagation}{equation.2.23}{}}
\newlabel{eq:delta-terms}{{25}{9}{Back-propagation}{equation.2.25}{}}
\newlabel{eq:weight-update}{{26}{9}{Back-propagation}{equation.2.26}{}}
\citation{kingma2014adam}
\citation{duchi2011adaptive}
\citation{hinton2012rmsprop}
\citation{QIAN1999145}
\citation{ioffe2015batch}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Adam optimizer}{10}{subsection.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Batch normalization}{10}{subsection.2.8}\protected@file@percent }
\newlabel{eq:batch-norm}{{33}{11}{Batch normalization}{equation.2.33}{}}
\citation{hebb1949}
\@writefile{toc}{\contentsline {section}{\numberline {3}Biologically plausible deep learning}{12}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Biological neurons}{12}{subsection.3.1}\protected@file@percent }
\newlabel{sec:neurons}{{3.1}{12}{Biological neurons}{subsection.3.1}{}}
\citation{Bi10464}
\citation{BengioLBL15}
\citation{Eric2018}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of biological neuron by BruceBlaus - Own work, CC BY 3.0 \footnotemark \relax }}{13}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:bio-neuron}{{1}{13}{Illustration of biological neuron by BruceBlaus - Own work, CC BY 3.0 \protect \footnotemark \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Biological constraints}{13}{subsection.3.2}\protected@file@percent }
\newlabel{sec:biological-constraints}{{3.2}{13}{Biological constraints}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Biological violations of backprop and deep learning}{13}{subsubsection.3.2.1}\protected@file@percent }
\citation{Hodgkin1952}
\citation{Boht2000SpikePropBF}
\citation{eshraghian2021training}
\citation{Eric2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Spiking neural networks}{15}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Training spiking neural networks}{15}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Leaky-Integrate-and-Fire neurons}{15}{subsubsection.3.3.2}\protected@file@percent }
\citation{hunsberger2015spiking}
\newlabel{fig:rate-function}{{2a}{16}{Interleaved plot of original rate function, $r(j)$ (orange), and smoothed version, $r_\rho (j)$ (blue).\relax }{figure.caption.4}{}}
\newlabel{sub@fig:rate-function}{{a}{16}{Interleaved plot of original rate function, $r(j)$ (orange), and smoothed version, $r_\rho (j)$ (blue).\relax }{figure.caption.4}{}}
\newlabel{fig:poisson-rate}{{2b}{16}{Theoretical (orange) and simulated spike rates (blue) as functions of input current.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:poisson-rate}{{b}{16}{Theoretical (orange) and simulated spike rates (blue) as functions of input current.\relax }{figure.caption.4}{}}
\newlabel{fig:rate-scaling}{{2c}{16}{Scaling of the rate function in the interval $[0; 10]$ to mimic ReLU.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:rate-scaling}{{c}{16}{Scaling of the rate function in the interval $[0; 10]$ to mimic ReLU.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plots of smoothed, poisson coded and ReLU-like rate functions, respectively.\relax }}{16}{figure.caption.4}\protected@file@percent }
\newlabel{eq:soft-rate}{{38}{16}{Leaky-Integrate-and-Fire neurons}{equation.3.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Poisson rate coding}{16}{subsubsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Low-pass alpha-filter}{17}{subsubsection.3.3.4}\protected@file@percent }
\newlabel{eq:alpha-filter}{{40}{17}{Low-pass alpha-filter}{equation.3.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Shadow training spiking neural networks}{17}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Activation scaling}{17}{subsubsection.3.4.1}\protected@file@percent }
\newlabel{sec:activation-scaling}{{3.4.1}{17}{Activation scaling}{subsubsection.3.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Rate estimation using alpha-filter}{17}{subsubsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Fusing batch normalization}{17}{subsubsection.3.4.3}\protected@file@percent }
\citation{raoballard1999}
\citation{millidge2021predictive}
\citation{PredictiveCodingNetworks}
\citation{PredictiveCodingNetworks}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.4}Testing spiking neural networks}{18}{subsubsection.3.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Predictive Coding}{18}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Predictive coding networks and inference learning}{18}{subsubsection.3.5.1}\protected@file@percent }
\newlabel{eq:pc-error-nodes-and-preds}{{43}{18}{Predictive coding networks and inference learning}{equation.3.43}{}}
\citation{PredictiveCodingNetworks}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Predictive coding network with 2 input nodes, 3 hidden nodes and 2 output nodes.\relax }}{19}{figure.caption.5}\protected@file@percent }
\newlabel{fig:predictive-coding-network}{{3}{19}{Predictive coding network with 2 input nodes, 3 hidden nodes and 2 output nodes.\relax }{figure.caption.5}{}}
\newlabel{eq:update-value-nodes}{{45}{19}{Predictive coding networks and inference learning}{equation.3.45}{}}
\newlabel{eq:il-weight-update}{{46}{19}{Predictive coding networks and inference learning}{equation.3.46}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Z-IL and equivalence to back-propagation}{20}{subsubsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3}Predictive coding networks for classification}{20}{subsubsection.3.5.3}\protected@file@percent }
\newlabel{eq:value-node-derivative}{{48}{20}{Predictive coding networks for classification}{equation.3.48}{}}
\citation{ioffe2015batch}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.4}Batch normalization in predictive coding networks}{21}{subsubsection.3.5.4}\protected@file@percent }
\citation{milladge2020predictive}
\citation{salvatori2021reverse}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results and Discussion}{22}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces First 10 examples of the MNIST training dataset.\relax }}{22}{figure.caption.6}\protected@file@percent }
\newlabel{fig:MNIST}{{4}{22}{First 10 examples of the MNIST training dataset.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experiment 1 - Z-IL vs. Backprop}{22}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Test accuracy and cross-entropy loss as function of number of parameter update steps for backprop and Z-IL.\relax }}{22}{figure.caption.7}\protected@file@percent }
\newlabel{fig:Z-IL-backprop-comparison}{{5}{22}{Test accuracy and cross-entropy loss as function of number of parameter update steps for backprop and Z-IL.\relax }{figure.caption.7}{}}
\citation{ioffe2015batch}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Experiment 2 - SPCN performance on MNIST digits}{23}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Neuron activity in each hidden and output layer of SNN presented with an image of the digit 7 for 200ms. For visualisation purposes each hidden layer only contains 32 neurons.\relax }}{23}{figure.caption.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Test accuracies on the P-MNIST benchmark.\relax }}{23}{table.caption.9}\protected@file@percent }
\newlabel{fig:snn-accuracies}{{1}{23}{Test accuracies on the P-MNIST benchmark.\relax }{table.caption.9}{}}
\citation{PredictiveCodingNetworks}
\citation{ContinualEqProp}
\citation{goodfellow2014generative}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Evaluation of Biological Plausibility}{24}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Benefits to biological plausible learning}{24}{subsection.4.4}\protected@file@percent }
\citation{milladge2020predictive}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{26}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Further research}{26}{subsection.5.1}\protected@file@percent }
\bibstyle{plain}
\bibdata{refs}
\bibcite{BengioLBL15}{1}
\bibcite{Bi10464}{2}
\bibcite{Boht2000SpikePropBF}{3}
\bibcite{duchi2011adaptive}{4}
\bibcite{EqProp}{5}
\bibcite{ContinualEqProp}{6}
\bibcite{eshraghian2021training}{7}
\bibcite{Goodfellow-et-al-2016}{8}
\bibcite{goodfellow2014generative}{9}
\bibcite{he2015delving}{10}
\bibcite{resnet}{11}
\bibcite{hebb1949}{12}
\bibcite{hinton2012rmsprop}{13}
\bibcite{lstm}{14}
\bibcite{Hodgkin1952}{15}
\bibcite{hunsberger2015spiking}{16}
\bibcite{Eric2018}{17}
\bibcite{ioffe2015batch}{18}
\bibcite{kingma2014adam}{19}
\bibcite{mnist}{20}
\bibcite{lillicrap2014random}{21}
\bibcite{lotter-prednet}{22}
\bibcite{millidge2021predictive}{23}
\bibcite{milladge2020predictive}{24}
\bibcite{noekland2016direct}{25}
\bibcite{QIAN1999145}{26}
\bibcite{raoballard1999}{27}
\bibcite{rumelhart1986learning}{28}
\bibcite{salvatori2021reverse}{29}
\bibcite{PredictiveCodingNetworks}{30}
\bibcite{transformers}{31}
\bibcite{predcoding}{32}
\citation{ioffe2015batch}
\@writefile{toc}{\contentsline {section}{\numberline {6}Appendix}{30}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Batch normalization derivatives}{30}{subsection.6.1}\protected@file@percent }
\newlabel{appendix:batch-norm-grad}{{6.1}{30}{Batch normalization derivatives}{subsection.6.1}{}}
\gdef \@abspage@last{34}
