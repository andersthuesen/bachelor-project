\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{lstm}
\citation{resnet}
\citation{transformers}
\citation{BengioLBL15}
\citation{EqProp}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Related work}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Research questions}{1}{subsection.1.2}\protected@file@percent }
\citation{mnist}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {section}{\numberline {2}Classical deep learning}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Mathematical notation}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Dataset}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Maximum Likelihood Estimation}{2}{subsection.2.3}\protected@file@percent }
\newlabel{eq:negative-log-likelihood}{{4}{3}{Maximum Likelihood Estimation}{equation.2.4}{}}
\newlabel{eq:softmax}{{7}{3}{Maximum Likelihood Estimation}{equation.2.7}{}}
\newlabel{eq:softmax-derivative}{{12}{4}{Maximum Likelihood Estimation}{equation.2.12}{}}
\newlabel{eq:partial-log-softmax}{{15}{4}{Maximum Likelihood Estimation}{equation.2.15}{}}
\citation{he2015delving}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Feedforward neural networks}{5}{subsection.2.4}\protected@file@percent }
\citation{rumelhart1986learning}
\citation{noekland2016direct}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Weight initialization}{6}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Back-propagation}{6}{subsection.2.5}\protected@file@percent }
\newlabel{sec:backprop}{{2.5}{6}{Back-propagation}{subsection.2.5}{}}
\newlabel{eq:dLdW1}{{23}{6}{Back-propagation}{equation.2.23}{}}
\citation{ioffe2015batch}
\newlabel{eq:delta-terms}{{25}{7}{Back-propagation}{equation.2.25}{}}
\newlabel{eq:weight-update}{{26}{7}{Back-propagation}{equation.2.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Batch normalization}{7}{subsection.2.6}\protected@file@percent }
\newlabel{eq:batch-norm}{{30}{7}{Batch normalization}{equation.2.30}{}}
\citation{hunsberger2015spiking}
\citation{PredictiveCodingNetworks}
\citation{hebb1949}
\@writefile{toc}{\contentsline {section}{\numberline {3}Biologically plausible deep learning}{8}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Biological neurons}{8}{subsection.3.1}\protected@file@percent }
\newlabel{sec:neurons}{{3.1}{8}{Biological neurons}{subsection.3.1}{}}
\citation{Bi10464}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of biological neuron by BruceBlaus - Own work, CC BY 3.0 \footnotemark \relax }}{9}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:bio-neuron}{{1}{9}{Illustration of biological neuron by BruceBlaus - Own work, CC BY 3.0 \protect \footnotemark \relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Biological constraints}{9}{subsection.3.2}\protected@file@percent }
\newlabel{sec:biological-constraints}{{3.2}{9}{Biological constraints}{subsection.3.2}{}}
\citation{BengioLBL15}
\citation{Eric2018}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Biological violations of backprop and deep learning}{10}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Spiking neural networks}{10}{subsection.3.3}\protected@file@percent }
\citation{Boht2000SpikePropBF}
\citation{eshraghian2021training}
\citation{Eric2018}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Training spiking neural networks}{11}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Leaky-Integrate-and-Fire neurons}{11}{subsubsection.3.3.2}\protected@file@percent }
\citation{hunsberger2015spiking}
\newlabel{fig:rate-function}{{2a}{12}{Interleaved plot of original rate function, $r(j)$ (orange), and smoothed version, $r_\rho (j)$ (blue).\relax }{figure.caption.4}{}}
\newlabel{sub@fig:rate-function}{{a}{12}{Interleaved plot of original rate function, $r(j)$ (orange), and smoothed version, $r_\rho (j)$ (blue).\relax }{figure.caption.4}{}}
\newlabel{fig:poisson-rate}{{2b}{12}{Theoretical (orange) and simulated spike rates (blue) as functions of input current.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:poisson-rate}{{b}{12}{Theoretical (orange) and simulated spike rates (blue) as functions of input current.\relax }{figure.caption.4}{}}
\newlabel{fig:rate-scaling}{{2c}{12}{Scaling of the rate function in the interval $[0; 10]$ to mimic ReLU.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:rate-scaling}{{c}{12}{Scaling of the rate function in the interval $[0; 10]$ to mimic ReLU.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plots of smoothed, poisson coded and ReLU-like rate functions, respectively.\relax }}{12}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Poisson rate coding}{12}{subsubsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Low-pass alpha-filter}{12}{subsubsection.3.3.4}\protected@file@percent }
\newlabel{eq:alpha-filter}{{37}{13}{Low-pass alpha-filter}{equation.3.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Shadow training spiking neural networks}{13}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Activation scaling}{13}{subsubsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Rate estimation using alpha-filter}{13}{subsubsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Fusing batch normalization}{13}{subsubsection.3.4.3}\protected@file@percent }
\citation{raoballard1999}
\citation{millidge2021predictive}
\citation{PredictiveCodingNetworks}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Predictive coding network with 2 input nodes, 3 hidden nodes and 2 output nodes.\relax }}{14}{figure.caption.5}\protected@file@percent }
\newlabel{fig:predictive-coding-network}{{3}{14}{Predictive coding network with 2 input nodes, 3 hidden nodes and 2 output nodes.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.4}Testing spiking neural networks}{14}{subsubsection.3.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Predictive Coding}{14}{subsection.3.5}\protected@file@percent }
\citation{PredictiveCodingNetworks}
\citation{PredictiveCodingNetworks}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Predictive coding networks and inference learning}{15}{subsubsection.3.5.1}\protected@file@percent }
\newlabel{eq:update-value-nodes}{{42}{15}{Predictive coding networks and inference learning}{equation.3.42}{}}
\newlabel{eq:il-weight-update}{{43}{15}{Predictive coding networks and inference learning}{equation.3.43}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Softmax and cross-entropy loss in inference learning}{15}{subsubsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3}Z-IL and equivalence to back-propagation}{16}{subsubsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.4}Predictive coding networks for classification}{16}{subsubsection.3.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.5}Biological plausability}{16}{subsubsection.3.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{16}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{16}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{16}{section.6}\protected@file@percent }
\bibstyle{plain}
\bibdata{refs}
\bibcite{BengioLBL15}{1}
\bibcite{Bi10464}{2}
\bibcite{Boht2000SpikePropBF}{3}
\bibcite{EqProp}{4}
\bibcite{eshraghian2021training}{5}
\bibcite{Goodfellow-et-al-2016}{6}
\bibcite{he2015delving}{7}
\bibcite{resnet}{8}
\bibcite{hebb1949}{9}
\bibcite{lstm}{10}
\bibcite{hunsberger2015spiking}{11}
\bibcite{Eric2018}{12}
\bibcite{ioffe2015batch}{13}
\bibcite{mnist}{14}
\bibcite{millidge2021predictive}{15}
\bibcite{noekland2016direct}{16}
\bibcite{raoballard1999}{17}
\bibcite{rumelhart1986learning}{18}
\bibcite{PredictiveCodingNetworks}{19}
\bibcite{transformers}{20}
\gdef \@abspage@last{20}
