\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{lstm}
\citation{resnet}
\citation{transformers}
\citation{hebb1949}
\citation{noekland2016direct}
\citation{lillicrap2014random}
\citation{BengioLBL15}
\citation{EqProp}
\citation{ContinualEqProp}
\citation{predcoding}
\citation{raoballard1999}
\citation{PredictiveCodingNetworks}
\citation{salvatori2021reverse}
\citation{milladge2020predictive}
\citation{lotter-prednet}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Related Work}{1}{subsection.1.1}\protected@file@percent }
\citation{Boht2000SpikePropBF}
\citation{eshraghian2021training}
\citation{hunsberger2015spiking}
\citation{hunsberger2015spiking}
\citation{PredictiveCodingNetworks}
\citation{BengioLBL15}
\citation{Eric2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Research Questions}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Project Outline}{3}{subsection.1.3}\protected@file@percent }
\citation{mnist}
\@writefile{toc}{\contentsline {section}{\numberline {2}Deep Learning}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Mathematical Notation}{4}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Dataset}{4}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Maximum Likelihood Estimation}{4}{subsection.2.3}\protected@file@percent }
\newlabel{sec:MLE}{{2.3}{4}{Maximum Likelihood Estimation}{subsection.2.3}{}}
\citation{Goodfellow-et-al-2016}
\newlabel{eq:negative-log-likelihood}{{4}{5}{Maximum Likelihood Estimation}{equation.2.4}{}}
\newlabel{eq:regression-loss-deriv}{{6}{5}{Maximum Likelihood Estimation}{equation.2.6}{}}
\newlabel{eq:softmax}{{7}{6}{Maximum Likelihood Estimation}{equation.2.7}{}}
\newlabel{eq:softmax-derivative}{{12}{6}{Maximum Likelihood Estimation}{equation.2.12}{}}
\newlabel{eq:partial-log-softmax}{{15}{7}{Maximum Likelihood Estimation}{equation.2.15}{}}
\newlabel{eq:cross-entropy-loss-deriv}{{22}{7}{Maximum Likelihood Estimation}{equation.2.22}{}}
\citation{he2015delving}
\citation{rumelhart1986learning}
\citation{noekland2016direct}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Feed-forward Neural Networks}{8}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Weight initialization}{8}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Back-propagation}{8}{subsection.2.5}\protected@file@percent }
\newlabel{sec:backprop}{{2.5}{8}{Back-propagation}{subsection.2.5}{}}
\newlabel{eq:dLdW1}{{23}{9}{Back-propagation}{equation.2.23}{}}
\newlabel{eq:delta-terms}{{25}{9}{Back-propagation}{equation.2.25}{}}
\newlabel{eq:weight-update}{{26}{9}{Back-propagation}{equation.2.26}{}}
\citation{ioffe2015batch}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Batch normalization}{10}{subsection.2.6}\protected@file@percent }
\newlabel{eq:batch-norm}{{30}{10}{Batch normalization}{equation.2.30}{}}
\citation{hebb1949}
\@writefile{toc}{\contentsline {section}{\numberline {3}Biologically plausible deep learning}{11}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Biological neurons}{11}{subsection.3.1}\protected@file@percent }
\newlabel{sec:neurons}{{3.1}{11}{Biological neurons}{subsection.3.1}{}}
\citation{Bi10464}
\citation{BengioLBL15}
\citation{Eric2018}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of biological neuron by BruceBlaus - Own work, CC BY 3.0 \footnotemark \relax }}{12}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:bio-neuron}{{1}{12}{Illustration of biological neuron by BruceBlaus - Own work, CC BY 3.0 \protect \footnotemark \relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Biological constraints}{12}{subsection.3.2}\protected@file@percent }
\newlabel{sec:biological-constraints}{{3.2}{12}{Biological constraints}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Biological violations of backprop and deep learning}{12}{subsubsection.3.2.1}\protected@file@percent }
\citation{Hodgkin1952}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Spiking neural networks}{13}{subsection.3.3}\protected@file@percent }
\citation{Boht2000SpikePropBF}
\citation{eshraghian2021training}
\citation{Eric2018}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Training spiking neural networks}{14}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Leaky-Integrate-and-Fire neurons}{14}{subsubsection.3.3.2}\protected@file@percent }
\citation{hunsberger2015spiking}
\newlabel{fig:rate-function}{{2a}{15}{Interleaved plot of original rate function, $r(j)$ (orange), and smoothed version, $r_\rho (j)$ (blue).\relax }{figure.caption.5}{}}
\newlabel{sub@fig:rate-function}{{a}{15}{Interleaved plot of original rate function, $r(j)$ (orange), and smoothed version, $r_\rho (j)$ (blue).\relax }{figure.caption.5}{}}
\newlabel{fig:poisson-rate}{{2b}{15}{Theoretical (orange) and simulated spike rates (blue) as functions of input current.\relax }{figure.caption.5}{}}
\newlabel{sub@fig:poisson-rate}{{b}{15}{Theoretical (orange) and simulated spike rates (blue) as functions of input current.\relax }{figure.caption.5}{}}
\newlabel{fig:rate-scaling}{{2c}{15}{Scaling of the rate function in the interval $[0; 10]$ to mimic ReLU.\relax }{figure.caption.5}{}}
\newlabel{sub@fig:rate-scaling}{{c}{15}{Scaling of the rate function in the interval $[0; 10]$ to mimic ReLU.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plots of smoothed, poisson coded and ReLU-like rate functions, respectively.\relax }}{15}{figure.caption.5}\protected@file@percent }
\newlabel{eq:soft-rate}{{35}{15}{Leaky-Integrate-and-Fire neurons}{equation.3.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Poisson rate coding}{15}{subsubsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Low-pass alpha-filter}{15}{subsubsection.3.3.4}\protected@file@percent }
\newlabel{eq:alpha-filter}{{37}{16}{Low-pass alpha-filter}{equation.3.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Shadow training spiking neural networks}{16}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Activation scaling}{16}{subsubsection.3.4.1}\protected@file@percent }
\newlabel{sec:activation-scaling}{{3.4.1}{16}{Activation scaling}{subsubsection.3.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Rate estimation using alpha-filter}{16}{subsubsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Fusing batch normalization}{16}{subsubsection.3.4.3}\protected@file@percent }
\citation{raoballard1999}
\citation{millidge2021predictive}
\citation{PredictiveCodingNetworks}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Predictive coding network with 2 input nodes, 3 hidden nodes and 2 output nodes.\relax }}{17}{figure.caption.6}\protected@file@percent }
\newlabel{fig:predictive-coding-network}{{3}{17}{Predictive coding network with 2 input nodes, 3 hidden nodes and 2 output nodes.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.4}Testing spiking neural networks}{17}{subsubsection.3.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Predictive Coding}{17}{subsection.3.5}\protected@file@percent }
\citation{PredictiveCodingNetworks}
\citation{PredictiveCodingNetworks}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Predictive coding networks and inference learning}{18}{subsubsection.3.5.1}\protected@file@percent }
\newlabel{eq:pc-error-nodes-and-preds}{{40}{18}{Predictive coding networks and inference learning}{equation.3.40}{}}
\newlabel{eq:update-value-nodes}{{42}{18}{Predictive coding networks and inference learning}{equation.3.42}{}}
\newlabel{eq:il-weight-update}{{43}{18}{Predictive coding networks and inference learning}{equation.3.43}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Z-IL and equivalence to back-propagation}{19}{subsubsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3}Predictive coding networks for classification}{19}{subsubsection.3.5.3}\protected@file@percent }
\citation{milladge2020predictive}
\citation{salvatori2021reverse}
\citation{ioffe2015batch}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results and Discussion}{20}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Experiment 1 - Z-IL vs. Backprop}{20}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Test accuracy and cross-entropy loss as function of number of parameter update steps for backprop and Z-IL.\relax }}{20}{figure.caption.7}\protected@file@percent }
\newlabel{fig:Z-IL-backprop-comparison}{{4}{20}{Test accuracy and cross-entropy loss as function of number of parameter update steps for backprop and Z-IL.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Experiment 2 - Shadow training spiking neural networks}{20}{subsection.4.2}\protected@file@percent }
\citation{PredictiveCodingNetworks}
\citation{ContinualEqProp}
\citation{goodfellow2014generative}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Neuron activity in each hidden and output layer of SNN presented with an image of the digit 7 for 200ms. For visualisation purposes each hidden layer only contains 32 neurons.\relax }}{21}{figure.caption.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Test accuracies on the P-MNIST benchmark.\relax }}{21}{table.caption.9}\protected@file@percent }
\newlabel{fig:snn-accuracies}{{1}{21}{Test accuracies on the P-MNIST benchmark.\relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Evaluation of Biological Plausibility}{21}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Benefits to biological plausible learning}{22}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{23}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Further research}{23}{subsection.5.1}\protected@file@percent }
\bibstyle{plain}
\bibdata{refs}
\bibcite{BengioLBL15}{1}
\bibcite{Bi10464}{2}
\bibcite{Boht2000SpikePropBF}{3}
\bibcite{EqProp}{4}
\bibcite{ContinualEqProp}{5}
\bibcite{eshraghian2021training}{6}
\bibcite{Goodfellow-et-al-2016}{7}
\bibcite{goodfellow2014generative}{8}
\bibcite{he2015delving}{9}
\bibcite{resnet}{10}
\bibcite{hebb1949}{11}
\bibcite{lstm}{12}
\bibcite{Hodgkin1952}{13}
\bibcite{hunsberger2015spiking}{14}
\bibcite{Eric2018}{15}
\bibcite{ioffe2015batch}{16}
\bibcite{mnist}{17}
\bibcite{lillicrap2014random}{18}
\bibcite{lotter-prednet}{19}
\bibcite{millidge2021predictive}{20}
\bibcite{milladge2020predictive}{21}
\bibcite{noekland2016direct}{22}
\bibcite{raoballard1999}{23}
\bibcite{rumelhart1986learning}{24}
\bibcite{salvatori2021reverse}{25}
\bibcite{PredictiveCodingNetworks}{26}
\bibcite{transformers}{27}
\bibcite{predcoding}{28}
\gdef \@abspage@last{28}
